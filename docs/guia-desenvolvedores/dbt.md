## O que √© o DBT e para que serve?

Em um _pipeline_ ELT, os dados brutos s√£o extra√≠dos (_extract_) de uma fonte e carregados (_load_) em uma _data warehouse_ (no nosso caso: o _BigQuery Storage_). Em seguida, os dados brutos s√£o transformados em tabelas utiliz√°veis, usando consultas **SQL** executadas sobre os dados estocados nessa _data warehouse_.

O **dbt** (_Data Build Tool_) √© uma ferramenta de linha de comando que oferece uma maneira f√°cil para que analistas e engenheiros de dados criem, transformem e validem os dados em um _data warehouse_ de maneira mais eficiente. O **dbt** faz o T nos processos ELT (Extrair, Carregar, Transformar) ilustrados na figura abaixo.

![Fluxo de Dados com DBT](../static/img/tutoriais/dbt/fluxo_dbt.png)

No **dbt**, trabalhamos com modelos, que nada mais √© que um arquivo **SQL** com uma instru√ß√£o `select`. Esses modelos podem depender de outros modelos, ter testes definidos neles e podem ser criados como tabelas ou visualiza√ß√µes.

Fazendo um paralelo com o processo de ELT do Escrit√≥rio de Dados: utilizamos _pipelines_ como `dump_db` ou `dump_url` para extrair os dados de uma base **SQL** ou uma planilha Google e subir esses dados no _Google Cloud Storage_. Ap√≥s esse passo, os dados j√° est√£o dispon√≠veis para consulta, visualiza√ß√£o, constru√ß√£o de dashboards e todas as fun√ß√µes e facilitadores do _BigQuery_ podem ser aplicadas nos dados. Por√©m, na grande maioria das vezes, os dados brutos, diretos da fonte de dados original, precisam ser tratados antes de serem efetivamente utilizados.

Alguns exemplos de tratamento comumente feitos s√£o:

- Tipagem de colunas (`FLOAT64`, `STRING`, `GEOMETRY`, `INT64`, etc.).
- (Re)nomea√ß√£o de colunas.
- Mapeamento de _seed values_ (troca de `id_bairro` por `nome_bairro` ou vice-versa).
- Cria√ß√£o de novas colunas a partir dos pr√≥prios dados brutos.
- Complementa√ß√£o de dados a partir de outras bases que est√£o no mesmo _data warehouse_ (_Google Cloud Storage_).

## Escrevendo seu primeiro modelo DBT

O primeiro passo para escrever um modelo **dbt** √© identificar qual o _project_id_ em que as tabelas tratadas ser√£o materializadas. Na maioria dos casos, ser√° o mesmo _project_id_ em que os dados brutos foram armazenados em _staging_. Uma vez identificado o _project_id_, deve-se clonar o reposit√≥rio de nome `queries-` + `'project_id'`, onde o modelo ser√° constru√≠do. (Exemplos de reposit√≥rios: `queries-rj-smfp`, `queries-datario`, `queries-rj-sme`, etc.).

Uma vez que o reposit√≥rio foi clonado e se est√° em uma nova _branch_, deve-se criar uma nova pasta dentro do diret√≥rio `queries-(...).models` com um nome breve que identifique as tabelas que ser√£o materializadas. Exemplos: `dashboard_metas`, `educacao_basica_avaliacao`, ou o pr√≥prio _dataset_id_ das tabelas que ser√£o materializadas. Aqui, utilizarei `test_formacao`.

Dentro dessa pasta, ficam os modelos de dados, que nada mais s√£o que arquivos **SQL** com uma instru√ß√£o `select`. Al√©m disso, os nomes dos modelos criados pelo **dbt** s√£o os pr√≥prios nomes dos arquivos, ent√£o nomeie os arquivos cuidadosamente. Seguem abaixo 2 arquivos, `elementos.sql` e `paises_americanos.sql`, como exemplos de modelos.

=== "elementos.sql"

```sql
SELECT
  SAFE_CAST(data as DATE) as data,
  SAFE_CAST(number as INT64) as numero,
  element as elemento
FROM `rj-escritorio-dev.test_formacao_staging.test_table_2`
```

=== "paises_americanos.sql"

```sql
SELECT
  LPAD(id_pais, 2, '0') as id_pais,
  CASE
    WHEN continente = 'Am√©rica do Sul' THEN '01'
    WHEN continente = 'Am√©rica Central' THEN '02'
    WHEN continente = 'Am√©rica do Norte' THEN '03'
    ELSE '00'
  END as id_continente,
  pais,
  capital
FROM `rj-escritorio-dev.test_formacao_staging.test_table`
```

No modelo `elementos`, foi realizada uma tipagem nas colunas de data e n√∫mero, al√©m de uma renomea√ß√£o das colunas de ingl√™s para portugu√™s. No modelo `paises_americanos`, foi utilizada a fun√ß√£o `LPAD` para que todos os _id_pais_ ficassem com tamanho 2, al√©m da cria√ß√£o de uma nova coluna de _id_continente_.

Note que as _queries_ acima rodam no pr√≥prio console do _BigQuery_, inclusive, foram desenvolvidas e testadas l√°. Lembre-se de testar seu c√≥digo **SQL** antes de subir um _pull request_ com um novo modelo.

Uma vez criados os modelos, √© necess√°rio criar o arquivo `schema.yml`, que cont√©m seus metadados, como a descri√ß√£o do modelo (tabela), o nome das colunas e suas descri√ß√µes. Segue abaixo o `schema.yml` para os modelos criados acima:

=== "schema.yml"

```sql
version: 2
models :
  - name: elementos
    description: "**Descri√ß√£o**: Tabelas com os 4 elementos da natureza, seus n√∫meros e datas aleat√≥rias.\
    Utilizado como exemplo para a Forma√ß√£o Infra 2022 do Escrit√≥rio de Dados do Rio de Janeiro\n\
    **Frequ√™ncia de atualiza√ß√£o**: N√£o possui"
    columns:
      - name: data
        description: Data em que os dados da linha foram inseridos.
      - name: numero
        description: C√≥digo do elemento.
      - name: elemento
        description: Nome do elemento, em ingl√™s.
  - name: paises_americanos
    description: "**Descri√ß√£o**: Tabelas com todos os pa√≠ses da Am√©rica e informa√ß√µes sobre eles. \
    Utilizado como exemplo para a Forma√ß√£o Infra 2022 do Escrit√≥rio de Dados do Rio de Janeiro\n\
    **Frequ√™ncia de atualiza√ß√£o**: N√£o possui"
    columns:
      - name: id_pais
        description: C√≥digo do pa√≠s.
      - name: id_continente
        description: C√≥digo do continente a qual o pa√≠s pertence.
      - name: pais
        description: Nome do pa√≠s.
      - name: capital
        description: Capital do pa√≠s.
```

üö®üö® **Aten√ß√£o** üö®üö®: Essas descri√ß√µes t√™m que ser exatamente as mesmas que foram preenchidas no arquivo de arquitetura e no [meta.dados.rio](meta.dados.rio/).

Ap√≥s criar os modelos e o arquivo _schema_, o √∫ltimo passo √© alterar o arquivo `dbt_project.yml`, que fica na base do reposit√≥rio. No final do arquivo h√° um trecho com as keys `models:` e `project_id:` do seu projeto no qual s√£o definidos os modelos (queries) que o dbt dever√° executar. √â nessa parte que deve-se adicionar o nome da pasta que voc√™ criou com os novos modelos (nesse caso, `exemplo_formacao_infra`) e o tipo de materializa√ß√£o para os modelos daquela pasta (_view_, _table_ or _incremental_). Nesse caso, utilizaremos _table_. (Aqui tem um [exemplo](https://github.com/prefeitura-rio/queries-rj-smfp/blob/master/dbt_project.yml) desse arquivo totalmente preenchido).

=== "dbt_project.yml"

```sql
(...)
models:
  +persist_docs:
    relation: true
    columns: true
  rj_escritorio:
    test_formacao:
      +materialized: table
      +schema: test_formacao
```

üö®üö® **Aten√ß√£o** üö®üö®: A identa√ß√£o de cada uma das partes desses arquivos √© fundamental para o correto funcionamento do DBT. Mantenha sempre o padr√£o de identa√ß√£o.

E √© isso! Se voc√™ chegou at√© aqui voc√™ j√° criou seu primeiro modelo **dbt**.

Para se aprofundar mais sobre esse tema, acesse a documenta√ß√£o oficial do [DBT](https://docs.getdbt.com/reference/model-configs.).

## Parametrizando queries

Em muitos casos, existem par√¢metros relevantes para o funcionamento das queries cuja mudan√ßa vai alterar drasticamente o resultado, como:

- Voc√™ coloca a taxa de imposto de algo e o governo muda esse valor, a √∫nica forma de mudar isso √© mudar o c√≥digo, achar todas as ocorr√™ncias desse valor na sua query e alterar um por um;

- Uma fun√ß√£o trabalha com 10 itens em algo que ela recebe, mas em algum momento h√° uma mudan√ßa e vem 12 itens que voc√™ deveria avaliar, ent√£o √© necess√°rio repensar todas as l√≥gicas realizadas e fazer as substitui√ß√µes;

- Uma query est√° filtrada para pegar registros dos √∫ltimos 3 anos, mas agora houve uma mudan√ßa e √© necess√°rio pegar dados dos √∫ltimos 5 anos, ent√£o voc√™ tem que substituir esse valor todas as vezes em que ele foi referenciado.

Os casos acima s√£o exemplos de _hard code_, que significa que uma informa√ß√£o relevante para o funcionamento do sistema foi colocado no c√≥digo. Isso resulta em muito trabalho pra fazer altera√ß√µes, dificultando a reprodu√ß√£o do c√≥digo e at√© a pr√≥pria (re)utiliza√ß√£o do mesmo por outras pessoas.

Felizmente, o **dbt** tem uma solu√ß√£o pra esse problema! √â poss√≠vel declarar vari√°veis no arquivo `dbt_project.yml` e se referenciar √† elas dentro das queries, de modo que quando algum valor dessas vari√°veis mudar, tudo que precisa ser feito √© alterar esse valor uma √∫nica vez.

Continuando o exemplo que foi feito acima, suponhamos que seja necess√°rio selecionar elementos da natureza, no modelo `elementos.sql` que foram declarados antes de uma certa data, e al√©m disso, que queiramos criar uma nova coluna que indica se um pa√≠s americano tem a capital que inicia com determinada letra, no modelo `paises_americanos.sql`. O primeiro passo √© declarar os valores dessas vari√°veis dentro do arquivo `dbt_project.yml`:

=== "dbt_project.yml"

```sql
(...)
vars:
  ELEMENTOS_DATA_INICIAL: "2022-01-03"
  PAISES_AMERICANOS_LETRA_INICIAL_CAPITAL: "B"

models:
  +persist_docs:
    relation: true
    columns: true
  rj_escritorio:
    test_formacao:
      +materialized: table
      +schema: test_formacao
```

Depois, inclu√≠mos essas vari√°veis dentro da query, referenciando-as com `{{var('NOME_DA_VARIAVEL')}}`:

=== "elementos.sql"

```sql
SELECT
  SAFE_CAST(data as DATE) as data,
  SAFE_CAST(number as INT64) as numero,
  element as elemento,
FROM `rj-escritorio-dev.test_formacao_staging.test_table_2`
WHERE data < "{{var('ELEMENTOS_DATA_INICIAL')}}"
```

=== "paises_americanos.sql"

```sql
SELECT
  LPAD(id_pais, 2, '0') as id_pais,
  CASE
    WHEN continente = 'Am√©rica do Sul' THEN '01'
    WHEN continente = 'Am√©rica Central' THEN '02'
    WHEN continente = 'Am√©rica do Norte' THEN '03'
    ELSE '00'
  END as id_continente,
  pais,
  capital,
  CASE
    WHEN STARTS_WITH(capital, "{{var('PAISES_AMERICANOS_LETRA_INICIAL_CAPITAL')}}")
      THEN CONCAT("Come√ßa com ", "{{var('PAISES_AMERICANOS_LETRA_INICIAL_CAPITAL')}}")
    ELSE CONCAT("N√£o come√ßa com ", "{{var('PAISES_AMERICANOS_LETRA_INICIAL_CAPITAL')}}")
    END indicador_capital_com_{{var('PAISES_AMERICANOS_LETRA_INICIAL_CAPITAL')}}
FROM `rj-escritorio-dev.test_formacao_staging.test_table`
```

Ainda sobre parametriza√ß√£o de queries, uma outra possibilidade √© a de referenciar outros modelos(queries) dentro de um modelo. Por exemplo, caso dentro do modelo `elementos.sql`, eu deseje fazer um `SELECT` no modelo `paises_americanos.sql`, isso √© poss√≠vel, basta referenciar esse modelo na forma `{{ ref('NOME_DO_MODELO') }}`.

=== "elementos.sql"

```sql
WITH select_paises AS (
  SELECT
    *
  FROM {{ ref('paises_americanos') }}
)

SELECT
  SAFE_CAST(data as DATE) as data,
  SAFE_CAST(number as INT64) as numero,
  element as elemento,
FROM `rj-escritorio-dev.test_formacao_staging.test_table_2`
WHERE data < "{{var('ELEMENTOS_DATA_INICIAL')}}"
```

O caso acima √© especialmente √∫til quando um modelo depende de outro pra ser executado corretamente. Nesse caso, √© necess√°rio que a query do modelo dependente seja executada depois da query do modelo indepentende. Isso √© poss√≠vel de configurar em uma pipeline do prefect atrav√©s dos par√¢metros `upstream` e `downstream`, que veremos mais abaixo.

## Integrando com as pipelines do Prefect

Para adicionarmos essa parte de materializa√ß√£o dos dados via DBT precisamos adicionar essa etapa no nosso `flows.py`. Isso vai depender se nossa pipeline foi criada atrav√©s de um flow pr√©-definido (como no caso de flows que acessam os bancos de dados da prefeitura ou planilhas do google sheets) ou se a iniciamos do zero.

### DBT para Flows pr√©-definidos

Vamos reutilizar os c√≥digos da aula anterior que extrai informa√ß√µes e um google sheets:

=== "flows.py"

```python
# -*- coding: utf-8 -*-
"""
Database dumping flows for formation project
"""

from copy import deepcopy

from prefect.run_configs import KubernetesRun
from prefect.storage import GCS

from pipelines.constants import constants

from pipelines.rj_escritorio.dump_db_formacao.schedules import gsheets_one_minute_update_schedule
from pipelines.utils.dump_url.flows import dump_url_flow  # alterado
from pipelines.utils.utils import set_default_parameters

formacao_gsheets_flow = deepcopy(dump_url_flow)
formacao_gsheets_flow.name = "EMD: Forma√ß√£o GSheets - Ingerir tabelas de URL"
formacao_gsheets_flow.storage = GCS(constants.GCS_FLOWS_BUCKET.value)
formacao_gsheets_flow.run_config = KubernetesRun(
    image=constants.DOCKER_IMAGE.value,
    labels=[
        constants.RJ_ESCRITORIO_DEV_AGENT_LABEL.value,
    ],
)

formacao_gsheets_flow_parameters = {
    "dataset_id": "test_formacao",
    "dump_mode": "overwrite",
    "url": "https://docs.google.com/spreadsheets/d/1uF-Gt5AyZmxCQQEaebvWF4ddRHeVuL6ANuoaY_-uAXE\
        /edit#gid=0",
    "url_type": "google_sheet",
    "gsheets_sheet_name": "sheet_1",
    "table_id": "test_table",
}

formacao_gsheets_flow = set_default_parameters(
    formacao_gsheets_flow, default_parameters=formacao_gsheets_flow_parameters
)

formacao_gsheets_flow.schedule = gsheets_one_minute_update_schedule

```

Dentro do `flows.py` precisamos apenas alterar a origem do flow que estamos copiando.

=== "schedule.py"

```python
# -*- coding: utf-8 -*-
"""
Schedules for the database dump pipeline
"""

from datetime import datetime, timedelta

import pytz
from prefect.schedules import Schedule
from pipelines.constants import constants
from pipelines.utils.dump_url.utils import generate_dump_url_schedules
from pipelines.utils.utils import untuple_clocks as untuple

#####################################
#
# EGPWeb Schedules
#
#####################################

gsheets_urls = {
    "test_table": {
        "url": "https://docs.google.com/spreadsheets/d/1uF-Gt5AyZmxCQQEaebvWF4ddRHeVuL6ANuoaY_-uAXE\
            /edit#gid=0",
        "url_type": "google_sheet",
        "gsheets_sheet_name": "sheet_1",
        "dump_mode": "overwrite", # alterado
        "materialize_after_dump": True, # alterado
        "materialization_mode": "dev", # alterado
        "materialize_to_datario": True, # alterado
        "dump_to_gcs": True, # alterado
    },
}


gsheets_clocks = generate_dump_url_schedules(
    interval=timedelta(minutes=1),
    start_date=datetime(2022, 10, 21, 15, 0, tzinfo=pytz.timezone("America/Sao_Paulo")),
    labels=[
        constants.RJ_ESCRITORIO_DEV_AGENT_LABEL.value,
    ],
    dataset_id="test_dataset_formacao",
    table_parameters=gsheets_urls,
)

gsheets_one_minute_update_schedule = Schedule(clocks=untuple(gsheets_clocks))

```

Agora temos 5 novas chaves dentro do nosso dicion√°rio de par√¢metros no arquivo `schedule.py`:

- **dump_mode**: ["overwrite", "append"] Define se os novos dados substituir√£o os dados anteriores no GCP (overwrite) ou se ser√£o adicionados (append) aos que j√° existem no GCP;

- **materialize_after_dump**: [True, False] Define se os dados ser√£o materializados no BQ (BigQuery) do pr√≥prio projeto utilizando o DBT;

- **materialization_mode**: ["dev", "prod"] Define se a materializa√ß√£o acontecer√° em ambiente de desenvolvimento ou produ√ß√£o;

- **materialize_to_datario**: [True, False] Define se os dados ser√£o materializados no datario utilizando o DBT.

- **dump_to_gcs**: [True, False] Define se os dados ser√£o salvos em um csv no Google Cloud Storage;

### DBT para Flows iniciados do zero

=== "flows.py"

```python
# -*- coding: utf-8 -*-
"""
Example flow
"""
from prefect import case, Parameter # adicionado
from prefect.run_configs import KubernetesRun
from prefect.storage import GCS
from prefect.tasks.prefect import create_flow_run, wait_for_flow_run # adicionado

from pipelines.formacao.exemplo.constants import (
    constants as formacao_constants,
)

from pipelines.constants import constants
from pipelines.formacao.exemplo.tasks import download_data, parse_data, save_report
from pipelines.utils.constants import constants as utils_constants # adicionado
from pipelines.utils.decorators import Flow
from pipelines.utils.dump_db.constants import constants as dump_db_constants # adicionado
from pipelines.utils.dump_to_gcs.constants import constants as dump_to_gcs_constants # adicionado
from pipelines.utils.tasks import ( # adicionado
    create_table_and_upload_to_gcs,
    get_current_flow_labels,
)

with Flow("EMD: formacao - Exemplo de flow do Prefect") as formacao_exemplo_flow:
    # Par√¢metros
    n_users = Parameter("n_users", default=10)

    # Par√¢metros para a Materializa√ß√£o
    materialize_after_dump = Parameter(
        "materialize_after_dump", default=False, required=False
    )
    materialize_to_datario = Parameter(
        "materialize_to_datario", default=False, required=False
    )
    materialization_mode = Parameter("mode", default="dev", required=False)

    # Par√¢metros para salvar dados no GCS
    dataset_id = formacao_constants.DATASET_ID.value
    table_id = formacao_constants.TABLE_ID.value
    dump_mode = "append"

    # Dump to GCS after? Should only dump to GCS if materializing to datario
    dump_to_gcs = Parameter("dump_to_gcs", default=False, required=False)

    maximum_bytes_processed = Parameter(
        "maximum_bytes_processed",
        required=False,
        default=dump_to_gcs_constants.MAX_BYTES_PROCESSED_PER_TABLE.value,
    )

    # Cria fluxo das Tasks
    data = download_data(n_users)
    dataframe = parse_data(data)
    save_report(dataframe)


    # Create table in BigQuery
    upload_table = create_table_and_upload_to_gcs(
        data_path=PATH,
        dataset_id=dataset_id,
        table_id=table_id,
        dump_mode=DUMP_MODE,
        wait=PATH,
    )

    # Trigger DBT flow run
    with case(materialize_after_dump, True):
        current_flow_labels = get_current_flow_labels()
        materialization_flow = create_flow_run(
            flow_name=utils_constants.FLOW_EXECUTE_DBT_MODEL_NAME.value,
            project_name=constants.PREFECT_DEFAULT_PROJECT.value,
            parameters={
                "dataset_id": dataset_id,
                "table_id": table_id,
                "mode": materialization_mode,
                "materialize_to_datario": materialize_to_datario,
            },
            labels=current_flow_labels,
            run_name=f"Materialize {dataset_id}.{table_id}",
        )

        materialization_flow.set_upstream(upload_table)

        wait_for_materialization = wait_for_flow_run(
            materialization_flow,
            stream_states=True,
            stream_logs=True,
            raise_final_state=True,
        )

        wait_for_materialization.max_retries = (
            dump_db_constants.WAIT_FOR_MATERIALIZATION_RETRY_ATTEMPTS.value
        )
        wait_for_materialization.retry_delay = timedelta(
            seconds=dump_db_constants.WAIT_FOR_MATERIALIZATION_RETRY_INTERVAL.value
        )

        with case(DUMP_TO_GCS, True):
            # Trigger Dump to GCS flow run with project id as datario
            dump_to_gcs_flow = create_flow_run(
                flow_name=utils_constants.FLOW_DUMP_TO_GCS_NAME.value,
                project_name=constants.PREFECT_DEFAULT_PROJECT.value,
                parameters={
                    "project_id": "datario",
                    "dataset_id": dataset_id,
                    "table_id": table_id,
                    "maximum_bytes_processed": MAXIMUM_BYTES_PROCESSED,
                },
                labels=[
                    "datario",
                ],
                run_name=f"Dump to GCS {dataset_id}.{table_id}",
            )
            dump_to_gcs_flow.set_upstream(wait_for_materialization)

            wait_for_dump_to_gcs = wait_for_flow_run(
                dump_to_gcs_flow,
                stream_states=True,
                stream_logs=True,
                raise_final_state=True,
            )


formacao_exemplo_flow.storage = GCS(constants.GCS_FLOWS_BUCKET.value)
formacao_exemplo_flow.run_config = KubernetesRun(
    image=constants.DOCKER_IMAGE.value,
    labels=[constants.RJ_COR_AGENT_LABEL.value],
)
formacao_exemplo_flow.schedule = None
```

=== "constants.py"

```python
# -*- coding: utf-8 -*-
"""
Constant values for the satelite tables
"""

from enum import Enum


class constants(Enum):
    """
    Constant values for the satelite project
    """

    DATASET_ID = "test_formacao"
    TABLE_ID = "test_table"
```

Modifica√ß√µes que precisamos fazer no arquivo `flows.py`:

- especificar um caminho no qual o dado ser√° salvo localmente antes de ser enviado ao GCP

- fazer o upload os dados no GCP utilizando o m√©todo `create_table_and_upload_to_gcs` e o caminho especificado anteriormente

- adicionar a condicional pronta (`with case`) que verificar√° se os dados ser√£o materializados
