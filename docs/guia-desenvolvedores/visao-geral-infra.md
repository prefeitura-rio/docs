E aÃ­, quer entender melhor nossa infraestrutura de dados, os componentes selecionados, como interagem
entre si e onde estÃ£o hospedados? EntÃ£o vamos lÃ¡! ðŸ‘¨â€ðŸ’»

## IntroduÃ§Ã£o ao fluxo de dados

NÃ£o hÃ¡ nada extravagante em nosso fluxo de dados, na realidade, ele Ã© tÃ£o intuitivo quanto pode ser.
No entanto, Ã© muito importante entender cada etapa para que seja possÃ­vel compreender o funcionamento
da infraestrutura como um todo.

EntÃ£o, vamos colocar aqui um diagrama que ajudarÃ¡ a entender por onde o dado transita e no que consistem
as etapas:

![Diagrama de etapas de processamento dos dados](../static/img/tutoriais/visao-geral-infra/visao-geral-sem-componentes.jpg)

Como estamos falando sobre a infraestrutura de dados, vamos focar Ãºnica e exclusivamente na parte
central desse diagrama, ou seja, as trÃªs grandes etapas - IngestÃ£o, TransformaÃ§Ã£o e AplicaÃ§Ã£o - e o
bloco central, o ArmazÃ©m de Dados.

### ArmazÃ©m de dados

Como o prÃ³prio nome jÃ¡ diz, Ã© o local onde os dados sÃ£o armazenados. No entanto, nÃ£o Ã© uma ferramenta
tÃ£o simples quanto o nome faz parecer, dado que Ã© o componente central do fluxo de dados e, portanto,
deve suportar uma grande variedade de operaÃ§Ãµes, alÃ©m de uma imensa capacidade de processamento e
armazenamento, visto que os dados podem ter tamanhos e prioridades diferentes.

Mais especificamente, uma ferramenta de armazÃ©m de dados para nossa infraestrutura deve possuir as
seguintes funcionalidades:

- **Capacidade de armazenamento e processamento "ilimitada"**: nÃ£o queremos nos preocupar em qual o tamanho do dado
  que vamos armazenar, nÃ³s o queremos e ponto!
- **OtimizaÃ§Ã£o de processamento via particionamento e correlatos**: para dados de grande volume, Ã©
  sempre interessante recorrer a mecanismos de particionamento e semelhantes para otimizar o processamento
  e, consequentemente, os custos de uma consulta.
- **Suporte a SQL**: nÃ³s queremos usar a linguagem de consulta mais popular e acessÃ­vel atÃ© o momento.
- **FunÃ§Ãµes de anÃ¡lise geoespacial**: por se tratar de dados da cidade sÃ£o, em maioria, geolocalizados.
  Assim, ter funcionalidades de anÃ¡lise geoespacial Ã© fundamental.
- **GovernanÃ§a simplificada**: ter uma interface amigÃ¡vel para gerir o armazÃ©m de dados Ã© fundamental.
- **ConexÃµes com plataformas de desenvolvimento de dashboards**: nÃ£o adianta ter dados se nÃ£o conseguimos
  visualizÃ¡-los ou usÃ¡-los.
- **Possibilidade de disponibilizar os dados publicamente**: afinal, queremos que a comunidade acesse!

### IngestÃ£o

A ingestÃ£o Ã© o ato de obter um dado e trazÃª-lo para seu ambiente. Ela consiste em duas subetapas: a
extraÃ§Ã£o e o armazenamento.

A **ExtraÃ§Ã£o** Ã© ato de adquirir o dado a partir de alguma fonte, como um banco de dados, uma API ou
um arquivo qualquer. A partir daÃ­, o **Armazenamento** Ã© a etapa de salvar esse dado em algum lugar
como, por exemplo, um computador ou um armazÃ©m de dados.

Se vocÃª parar para pensar, coisas rotineiras podem ser enxergadas como ingestÃ£o: ao consultar seu
contracheque, por exemplo, vocÃª estÃ¡ realizando uma extraÃ§Ã£o, ao requerer os dados no website de
consulta, e um armazenamento, ao baixÃ¡-lo para seu computador.

### TransformaÃ§Ã£o

A transformaÃ§Ã£o de dados Ã© um conceito extremamente abrangente, entÃ£o sua definiÃ§Ã£o deve ser suficientemente
ampla: modificar um dado buscando um objetivo. EntÃ£o, qualquer modificaÃ§Ã£o no dado original Ã© considerada
uma transformaÃ§Ã£o. Seguindo o exemplo do contracheque, uma possÃ­vel transformaÃ§Ã£o seria a conversÃ£o de
moeda, caso vocÃª precisasse de um contracheque em dÃ³lar.

Note que, apÃ³s a transformaÃ§Ã£o, uma nova etapa de armazenamento Ã© realizada. PorÃ©m, esse dado transformado
poderÃ¡ sobrescrever o original, ou seja, o dado original nÃ£o serÃ¡ mais acessÃ­vel, ou coexistir com ele.

### AplicaÃ§Ã£o

Essa Ã© a Ãºltima etapa de um fluxo de dados. Ela consiste em utilizar o dado para algum fim. Aqui se
encontram coisas como dashboards, aplicativos, chat bots e outros sendo, entÃ£o, um pouco fora do
escopo desse guia.

Mas seguindo nosso exemplo do contracheque, uma possÃ­vel aplicaÃ§Ã£o seria a utilizaÃ§Ã£o dos dados do
seu contracheque para a declaraÃ§Ã£o do IRPF.

### ConsideraÃ§Ãµes finais

Como Ã© possÃ­vel notar pelo prÃ³prio diagrama, Ã© possÃ­vel iterar nesse fluxo indefinidamente. EntÃ£o,
grandes projetos de dados nÃ£o usam somente um dado contido no armazÃ©m, mas mÃºltiplos, fazendo transformaÃ§Ãµes
para unÃ­-los e obter informaÃ§Ãµes mais ricas. TambÃ©m, as transformaÃ§Ãµes podem ocorrer indefinidamente,
atÃ© que se obtenha o resultado esperado.

## ApresentaÃ§Ã£o dos componentes

Vamos incrementar aquele diagrama!

![Diagrama de componentes](../static/img/tutoriais/visao-geral-infra/visao-geral-componentes.jpg)

Aqui temos os componentes que compÃµem a infraestrutura de dados. Na parte de "ExtraÃ§Ã£o de dados",
temos o [Prefect](https://prefect.io/) para realizar a extraÃ§Ã£o e [Google Cloud Storage](https://cloud.google.com/storage/)
[Google BigQuery](https://cloud.google.com/bigquery/) para armazenamento. Logo mais, vocÃª irÃ¡ entender
o que cada um faz e como foram selecionados.

JÃ¡ na parte de "TransformaÃ§Ãµes", temos ali o [DBT](https://www.getdbt.com/) para realizar as transformaÃ§Ãµes
e novamente o [Google BigQuery](https://cloud.google.com/bigquery/) para armazenamento.

### Como chegamos a isso

Muitos pontos foram levantados durante o levantamento das ferramentas. Dentre eles, selecionamos um
conjunto dos que julgamos mais adequados para nossa infraestrutura que, de forma simplificada, Ã© o
seguinte:

- **Escalonamento**: todos os componentes devem funcionar bem para cargas de trabalho
  pequenas, mas devem ter a possibilidade de suportar cargas de trabalho enormes.
- **Liberdade**: como estamos hospedando nossos serviÃ§os em nuvem, gostarÃ­amos de ter a possibilidade
  de transitar entre provedores com facilidade. Por isso, sempre que razoÃ¡vel, optamos por usar
  software livre.
- **Possibilidades**: poder usufruir por completo de uma linguagem de programaÃ§Ã£o, como Python, ou
  de consulta, como SQL, permite fazer trabalhos extremamente complexos e replicÃ¡veis.
- **DescentralizaÃ§Ã£o**: hospedar cargas de trabalho de todos os Ã³rgÃ£os da prefeitura em servidores
  separados permite um maior controle sobre os custos de cada Ã³rgÃ£o e dificulta condiÃ§Ãµes em que
  cargas de trabalho de um Ã³rgÃ£o poderia onerar outro.
- **Simplicidade**: menos Ã© mais. Um sistema com poucos componentes Ã© mais fÃ¡cil de manter e entender.

Claro que esses itens sÃ£o muito abstratos e, sabendo disso, construÃ­mos um instrumento de avaliaÃ§Ã£o
de serviÃ§os com diversos fatores quantificÃ¡veis e, a partir dele, conduzimos uma consulta pÃºblica
com vÃ¡rios provedores de serviÃ§os em nuvem, para que propusessem soluÃ§Ãµes para nosso problema.

ApÃ³s a anÃ¡lise, os componentes escolhidos foram os explicitados no diagrama acima. Agora vamos conversar
sobre cada um deles e explicar como eles funcionam.

### ExtraÃ§Ã£o (Prefect)

Como mencionado anteriormente, para realizar a extraÃ§Ã£o de dados, a ferramenta selecionada foi o
[Prefect](https://prefect.io/). Essa Ã© uma ferramenta de cÃ³digo aberto, que permite a automaÃ§Ã£o
de quaisquer tarefas atravÃ©s de sua API, em Python.

Por possuir uma interface muito simples de usar, Ã© extremamente fÃ¡cil modificar seu cÃ³digo para
usufruir de suas funcionalidades. Mas, para nÃ³s, alÃ©m da linda interface que ele possui, os dois
itens que mais foram convidativos para nÃ³s foram:

- **ConfianÃ§a**: Ã© muito fÃ¡cil lidar com falhas no Prefect, vocÃª nÃ£o vai quebrar todo seu fluxo
  de dados se algo ruim acontecer. AlÃ©m disso, em nossos testes preliminares, conseguimos executar
  mais de 10 mil pipelines por dia, consumindo poucos recursos, sem problemas.
- **Modelo hÃ­brido de execuÃ§Ã£o**: esse deve ser o item mais apelativo para todos que aderem o Prefect.
  O [modelo hÃ­brido](https://www.prefect.io/why-prefect/hybrid-model/) dele permite que vocÃª tenha o servidor
  hospedado em um local, seu cÃ³digo em outro e o ambiente de execuÃ§Ã£o em um terceiro. Existe uma clara
  separaÃ§Ã£o de papeis que garante a seguranÃ§a da execuÃ§Ã£o do cÃ³digo e, de tabela, uma completa descentralizaÃ§Ã£o:
  podemos executar pipelines de um Ã³rgÃ£o em um servidor, outro em outro e, ao mesmo tempo, conseguir
  enxergar todas elas em uma Ãºnica interface.

E quando eu falo sobre facilidade de uso, eu realmente quero dizer que Ã© muito fÃ¡cil. Olha sÃ³ esse
exemplo: suponha que vocÃª tenha um cÃ³digo de extraÃ§Ã£o de dados em Python puro, dÃ¡ sÃ³ uma olhada em
como portÃ¡-lo para o Prefect.

=== "Python puro"

    ```py
    def extrair_dados(url):
        response = requests.get(url)
        data = response.json()
        return data

    def montar_dataframe(data):
        df = pd.DataFrame(data)
        return df

    def salvar_dados(dataframe):
        dataframe.to_csv('dados.csv')

    def main():
        url = 'https://url.da.api.que.voce.quer.extrair'
        data = extrair_dados(url)
        dataframe = montar_dataframe(data)
        salvar_dados(dataframe)
    ```

=== "Prefect"

    ```py
    from prefect import task, Flow

    @task
    def extrair_dados(url):
        response = requests.get(url)
        data = response.json()
        return data

    @task
    def montar_dataframe(data):
        df = pd.DataFrame(data)
        return df

    @task
    def salvar_dados(dataframe):
        dataframe.to_csv('dados.csv')

    with Flow("main") as flow:
        url = 'https://url.da.api.que.voce.quer.extrair'
        data = extrair_dados(url=url)
        dataframe = montar_dataframe(data=data)
        salvar_dados(dataframe=dataframe)
    ```

Bom demais, nÃ©?

### TransformaÃ§Ãµes (DBT)

Aqui Ã© onde a galera do SQL se empolga. O [DBT](https://getdbt.com) Ã© uma ferramenta de cÃ³digo aberto,
que permite a definiÃ§Ã£o de modelos de dados em linguagem SQL. EntÃ£o, em outras palavras, vocÃª pode
escrever uma query e, usando o DBT, materializar os resultados dessa query em diferentes formas e,
o que Ã© mais legal, em diferentes plataformas.

Assim, com a mesma query, vocÃª pode criar tabelas e _views_ em diferentes plataformas. E nÃ£o sÃ³ isso,
as tabelas podem ser materializadas de maneiras diferentes, como, por exemplo, de forma incremental,
que consiste em adicionar novas linhas ao final de uma tabela.

AlÃ©m de tudo, vocÃª pode parametrizar suas queries para que elas sejam executadas com diferentes
valores de entrada. NÃ£o bastante, vocÃª pode especificar configuraÃ§Ãµes de particionamento, dentre
outras, diretamente no seu modelo de dados. DÃ¡ uma olhada sÃ³ nesse exemplo:

```sql
{{
    config(
        materialized='table',
        partition_by={
            "field": "data_particao",
            "data_type": "date",
            "granularity": "month",
        }
    )
}}

SELECT
    *
FROM `datario.dataset_id.table_id`
```

Pode relaxar, vocÃª nÃ£o precisa entender tudo desse modelo ainda, teremos guias para isso. Mas, apesar
de curto, tem muita coisa rolando nesse modelo aÃ­! Estamos especificando para ele que deve se materializar
em formato de tabela nativa com o `materialized='table'`, e estamos especificando que deve ser particionado
por data na coluna `data_particao` com uma granularidade mensal. PouquÃ­ssimas linhas e simplesmente
funciona!

### Armazenamento (Google Cloud Storage e Google BigQuery)

Eu sei o que vocÃª vai dizer:

> Mas vocÃª disse "sempre que razoÃ¡vel, optamos por usar software livre"

Sim, exatamente, sempre que razoÃ¡vel. PorÃ©m, no caso do armazenamento, principalmente, as ferramentas
apresentadas pela Google atenderam todos os requisitos que havÃ­amos levantado, alÃ©m de reduzirem o
nÃºmero de componentes para dois ao invÃ©s de mÃºltiplos open-source que terÃ­amos que manter em conjunto.
Ok, vamos voltar ao que viemos fazer: apresentar os componentes.

- **Google Cloud Storage** Ã© um serviÃ§o de armazenamento de objetos. EntÃ£o, atravÃ©s de uma API, Ã©
  possÃ­vel fazer upload de qualquer tipo de arquivo, configurar permissÃµes para ele e, caso tenha acesso,
  baixÃ¡-lo novamente. AlÃ©m disso, vocÃª pode utilizÃ¡-lo como um _data lake_ em sua definiÃ§Ã£o original -
  vocÃª pode armazenar arquivos na mesma estrutura de partiÃ§Ãµes Hive e obter seus dados particionados
  para consulta. VocÃª, entÃ£o, pode consultar seus dados com SQL no console do BigQuery.

- **Google BigQuery** Ã© um serviÃ§o de _data warehouse serverless_. Ele permite anÃ¡lises escalonÃ¡veis
  em petabytes (mil terabytes) de dados. Como mencionado anteriormente, vocÃª pode consumir dados de
  _data lakes_ hospedados no Google Cloud Storage mas, ao mesmo tempo, pode criar tabelas nativas,
  que possuem performance muito superior.

E aqui, entÃ£o, finalizamos a apresentaÃ§Ã£o de nossos componentes. Mas calma, ainda tem muita coisa
a ser dita aqui, vamos em frente.

## Como estÃ£o hospedados

> Explicar como as coisas sÃ£o hospedadas, falar da descentralizaÃ§Ã£o, escalabilidade, Kubernetes,
> repositÃ³rios, GitHub Actions, etc.

## ComunicaÃ§Ã£o e interaÃ§Ãµes

> Os componentes precisam se comunicar pra fazer com que tudo funcione. Mencionar como isso Ã© feito,
> falar do basedosdados que Ã© um pacote importante que a gente usa, etc. Falar do BigQuery, GCS,
> tabelas externas, etc.
